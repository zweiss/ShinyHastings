%
% File acl2012.tex
%
% Contact: Maggie Li (cswjli@comp.polyu.edu.hk), Michael White (mwhite@ling.osu.edu)
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt]{article}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{color}
\usepackage[utf8]{inputenc} % use ß
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[translate=babel,nogroupskip,nonumberlist]{glossaries}
\usepackage[style=authoryear-icomp, pagetracker=page, backend=bibtex, doi=false,isbn=false,url=false,dashed=false]{biblatex}
\usepackage{gb4e}

% Hyperref 
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

% Glosses
\setacronymstyle{long-short}
\newacronym{mh}{MH}{Metropolis Hastings}
\newacronym{mcmc}{MCMC}{Markov chain Monte Carlo}
\newacronym{hdi}{HDI}{Highest Density Interval}
%\newacronym{}{}{}
\makenoidxglossaries

% Use '&' instead of ', and' in citations with multiple authors
\renewcommand*{\finalnamedelim}{%
   \ifnumgreater{\value{liststop}}{2}{}{}%
   \addspace\&\space} 

\title{Shiny Hastings: An Illustration of the Effect of \\the Proposal Density on the Metropolis Hastings Algorithm}

\author{Zarah Leonie Weiß \\
  {\tt \href{mailto:zarah-leonie.weiss@student.uni-tuebingen.de}{zarah-leonie.weiss@student.uni-tuebingen.de}} \\
  }

\date{}
\bibliography{mh.bib}

\begin{document}

\maketitle
\begin{abstract}
TO READ: 
also cite paper for shiny
\end{abstract}

\section{Introduction}\label{sec:intro}
% ca. 1 page

% MH is a common MCMC algorithm used for distribution approximation
% special: needs not to sample from actual target distribution, sufficient to use function proportional to it
% i.e. often used for distributions that cannot be sampled directly from, e.g. with multiple parameters

\gls{mcmc} algorithms are used to approximate distributions by sampling from a target distribution. Unlike other sampling methods, \gls{mcmc} methods do not necessarily draw from the actual target distribution. Instead, a distribution function proportional to the target distribution is sufficient. Therefore, \gls{mcmc} algorithms are often employed for non-standard distributions from which direct sampling is too difficult or computationally costly, for example if the target distribution has multiple parameters \parencite{???}. 
The two most commonly known types of \gls{mcmc} algorithms are \textit{Gibbs sampling} and \textit{Metropolis Hastings}, of which the latter is subject of this article.

************************************
% often used in Bayesian approaches to approximate the posterior distribution, if other methods fail
% other methods = based on prior or likelihood or grid search (Kruschke)
% handy: sufficient to use distribution proportional to target distribution, i.e. denominator in bayes theorem can be ignored
This characteristic is particularly beneficial for Bayesian inference approaches, when it comes to calculating the posterior distribution $P(\theta|D)$. 

%Equation~\ref{eq:bayes-prop} displays Bayes' theorem omitting the denominator and shows that $P(\theta|D)$ is proportional to the prior distribution $P( \theta )$ multiplied by the likelihood $P( D | \theta )$.
%\begin{equation}\label{eq:bayes-prop}
%P( \theta | D ) \propto  P( D | \theta ) P( \theta )
%%\end{equation}
%By multiplying prior and likelihood, we may generate arbitrary posteriors not corresponding to densities known by statistics \parencite[][153]{jackman2009}. 


Using Bayes' theorem, displayed in Equation~\ref{eq:bayes}, the calculation of the actual posterior distribution can be computationally very costly due to the denominator containing an integral.
\begin{equation}\label{eq:bayes}
P( \theta | D ) =  \frac{P( D | \theta ) P( \theta )}{ \int_{\theta'} P( D | \theta' ) P( \theta' ) d \theta'}
\end{equation}
In fact, the normalizing constant can be solved easily only, if $\theta$ is a single discrete variable with a restricted domain size and if the prior distribution is a conjugate prior for the likelihood function \parencite{???}.
However, since \gls{mcmc} may as well sample from a distribution proportional to the target distribution, the denominator may be omitted, leading to the computationally easier formula in Equation~\ref{eq:bayes-prop}, in which the posterior distribution is shown to be proportional to the prior distribution $P( \theta )$ multiplied by the likelihood $P( D | \theta )$.
\begin{equation}\label{eq:bayes-prop}
P( \theta | D ) \propto  P( D | \theta ) P( \theta )
\end{equation}
************************************

% in order to approximate successfully, sample space needs to be explored 
% efficiency and effectiveness of this tuned by choice of proposal distribution and acceptance rate
% vast amount of literature on those two aspects of the algorithm
In order to approximate the target distribution successfully, it is crucial for \gls{mcmc} methods to explore the sample space as exhaustively as possible, while converging in a reasonable amount of time. The efficiency and effectiveness of \gls{mh} may be tuned by the choice of the \textit{proposal distribution} and the \textit{acceptance rate}. Therefore, a vast amount of literature discusses different possible variants of proposal densities and acceptance rates \parencite[e.g.][]{liu2001,jackman2009}. 
% here: focus on proposal distribution
% implemented a Shiny app that allows to run MH with varying proposal distributions
% use random walk proposal types and independence hastings
% user can compare results and get feeling for effect of proposal distribution on sample results
While these discussions mostly remain theoretical and only partially discuss concrete implementations, I implemented a web application using Shiny \parencite{shiny}, a web application framework for R.\footnote{\url{http://shiny.rstudio.com}.} The \textit{Shiny Hastings} app allows to run a \gls{mh} implementation using varying proposal distributions, that have been discussed in the literature. It was build to allow users to immediately experience the effect of different proposal distribution implementations on the \gls{mh} algorithm and is accessible via GitHub\footnote{\url{???}.} under \url{???}.

% Report structure
In the following section~\ref{sec:mh}, \gls{mcmc} methods and the \gls{mh} algorithm are introduced, focussing especially on two families of proposal densities: the \textit{random-walk metropolis} and \textit{independence metropolis}. After introducing the theoretical background and the general algorithm, the implementation of \gls{mh} in \textit{Shiny Hastings} is discussed in section~\ref{sec:mh}. The report closes with the conclusion in section~\ref{sec:conclusion}.





%\gls{mh} is a common \gls{mcmc} algorithm used to approximate distributions. This group of algorithms is particularly beneficial when it comes to estimate distributions that cannot be sampled from directly, because it is sufficient to compute a function that is proportional to the actual distribution. Therefore, they are often applied in cases where multiple parameters need to be estimated, i.e. the target distribution has multiple dimensions.
%In order to approximate a distribution via samples, it is necessary to ensure an exhaustive exploration of the sample space. This can be tuned by the \textsc{proposal distribution} and the \textit{acceptance ratio}. Therefore, the efficiency of an \gls{mh} implementation is mainly determined by the choice of proposal distribution and there has been an extensive discussion on the best choice for a proposal distribution, see for example \textcite{liu2001,jackman2009}. 

%% What (content)
%- shiny app performing MH for given random data set
%- choice of proposal density 
%- display of evaluation methods
%- allows to investigate effect of proposal density on MH algorithm's efficiency
%
% Bayesian analysis

%% Why (motivation)
%- metropolis hastings method to get posterior distribution by sampling
%- acceptance rate and proposal density important for efficiency of algorithm
%- important question: how to choose those?
%- here app to illustrate effect of choice of proposal density on efficiency
%
%% How (means)
%- use shiny app and jags 
%- report  on default parameters in base version:
%-- normal distribution
%-- random symmetric priors
%-- likelihood
%-- number of chains and iterations and burn-in to choose
%-- choice of proposal densities



\section{Theoretical Background}\label{sec:mh}
% ca. 1,5 pages
************************************


- mh / mcmc when to use? too many parameters for grid approximation (Kruschke:144)
- sample from mathematically defines distribution instead of experimentally produced samples (Kruschke:145)

While it is not necessary for \gls{mcmc} methods applied in Bayesian inference statistics, that the integral in Bayes' rule can be evaluated, as the denominator can be dropped, it is crucial, that both, prior and likelihood function can be computed easily for a given parameter $\theta$. Based on this, \gls{mcmc} returns an approximation of $P(\theta|D)$ in form of a large set of samples of $\theta$ values \parencite[][144]{kruschke2014}.


- history of MCMC reviewed in Robert \& Casella 2010 (Robert 2016:1)

-- MH algorithm (Jackman 2009:172)
i.e. generate ergodic Markov chains  wrt. posterior densities  (Jackman 2009:172)

- Monte Carlo methods efficient for many dimensions, unlike numerical methods (Hastings 1970:97)
************************************

% Introduction: what? & why bother?
As just mentioned in section~\ref{sec:intro}, \gls{mh} is a sort of \gls{mcmc} algorithm. 
Those algorithms may be used to approximate virtually any \textit{target} distribution via sampling, even if direct sampling from that target distribution is computationally not feasible \parencite[][105]{liu2001}. This is due to the fact that it suffices for the algorithms to sample from distributions that are proportional to the target distribution \parencite[][140]{jackman2009}.\footnote{Another method of distribution approximation without directly sampling from the actual target distribution is \textit{importance sampling}, where samples instead are drawn from a \textit{trial} distribution resembling the actual target distribution. However, it would be beyond the scope of this report to discuss this alternative approach, please see \textcite{liu2001,jackman2009,hastings1970} instead.} The target density is then characterized based on summary statistics such as mean, central tendency estimates or \gls{hdi}, calculated with the samples \parencite[][145]{kruschke2014}. 
An example for such a target distribution is the posterior distribution $P(\theta|data)$ in Bayesian inference statistics, which may be an arbitrary density not corresponding to any density known by statistics \parencite[][153]{jackman2009}.  


\subsection{MCMC}

How do \gls{mcmc} methods work? As their name suggests, they combine two prominent statistical principles: the Monte Carlo principle, a well known technique, that has already been used to solve deterministic and statistical problems in the 1870s \parencite[cf.][140]{jackman2009},  and the Markov chain property, named after Russian mathematician Andrey Markov (1856-1922) \parencite[][171]{jackman2009}: 

\begin{description}
\item[The Monte Carlo principle] states, that it is possible to draw conclusions about a random variable $\theta$ based on a large enough number of samples from its density $f(\theta)$ (\cite[cf.][133]{jackman2009}, \cite[][1]{robert2016}). 
\end{description}
The crucial notion ensuring the Monte Carlo principle is \textit{simulation consistency}. Informally speaking, due to the \textit{law of large numbers} the statistical summary of a large enough amount of samples from a density $f(\theta)$ is an estimate of a property of $\theta$, i.e. is 
\textit{simulation-consistent} \parencite[][134]{jackman2009}. 
************************************
For a formal definition of simulation consistency please consult \textcite[][138]{jackman2009}.
************************************
Due to this principle, it is possible to draw independent random samples from a distribution in order to characterize it, as long as the number of samples is sufficiently large. However, based on the Monte Carlo principle alone it is not possible to characterize a target distribution by sampling from \textit{another} distribution proportional to it. This is, where Markov chains come into play \parencite[][1]{robert2016}.
% Bridge to Markov chain
%Typically, those samples are independent. If they are not independent, we are dealing with Markov chains, which are introduced in the next paragraph.

\begin{description}
\item[The Markov chain property] states, that for a sequence of time-indexed random variables, all believes about a given variable are based solely on its immediate past \parencite[][172f]{jackman2009}.
\end{description} 

************************************
Put more formally, this means that given a collection of random variables $\theta^{(t)}$, where $t$ is a time index, the following holds: 
$Pr( \theta^{(t+a)} = y | \theta^{(s)} = x_s, s \leq t ) = P( \theta^{(t+a)} = y | \theta^{(t)} = x_t), \forall a > 0$. 

************************************
Every sequence satisfying this property is a Markov chain.\footnote{It should be mentioned, that for the purposes of this paper only discrete time Markov chains were considered, but not the more general continuous time Markov processes. For more information on those, please see \textcite[][172ff]{jackman2009}.} 

How is this combined with Monte Carlo methods?
Markov chains are stochastic processes. If they are constructed appropriately on the parameter space $\Theta$ of a target distribution, i.e. set of states $\theta$ might take as values, they are \textit{ergodic}. This means, they visit the potential values of $\theta$ in the parameter space proportional to the probability assigned to them by the target distribution \parencite[][172]{jackman2009}. Therefore, the iteration sequence of the Markov chain constitutes a representative sample of the target distribution by the Monte Carlo principle, although these samples are not independent from each other. In fact, \textit{ergodictiy} is a form of the law of large numbers \parencite[][171]{jackman2009}. Yet, using dependently drawn samples is less efficient than using truly independent samples, which is why \gls{mcmc} methods need more samples to return sufficient approximations than typical Monte Carlo methods.

************************************
In order to be suitable for \gls{mcmc} methods, Markov chains need to satisfy certain properties.

- Transition kernel: (Jackman 2009:173)
-- reformulate markov property as transition probability, that at step t, Markov chain will
	jump from $\theta^{(t-1)}$ to set $A$, given Markov chain is at $\theta^{(t-1)}$:
	$Pr(\theta^{(t)} \in A | \theta^{(t)-1}, \theta^{(t-2)}, \dots) = Pr(\theta^{(t)} \in A | \theta^{(t-1)}), \forall A \subset \Theta$
- in discrete case, transition probabilities representable as square matrix with positive entries, i.e. transition matrix K (Jackman 2009:173),
	where$ K_{ij} = Pr(\theta^{(t)} = j | \theta^{(t-1)} = i)$
-- this characterizes the transition Kernel of Markov chain (Jackman 2009:174)
	
- Stationary distribution (Jackman 2009:177)

- Irreducibility (also called ergodic):
-- Markov chain needs to be able to access every state theta in Theta at every state Jackman 2009:179
-- if it fulfils this, it is irreducible, i.e. if $\Theta$ is a communication class Jackman 2009:179 )
-- absorbing state / closed state: cannot reach any other state from that state Jackman 2009:179
-- accessible state: state which can be accessed by another state
-- communicating states are mutually accessible
-- communication class is formed by communicating states
- in irreducible states it does not matter where we start from, at some point we will reach
	every state
	
- Recurrence: 
-- state is transient  if return time may be infinite
-- state is recurrent, otherwise
-- if waiting time for return is finite, it is positive recurrent
-- an irreducible Markoc chain on discrete state space is either posititive reucrrent for all
	states or for none
-- all states are positive recurrent in an irreducible Markov chain on finite state space, 
	i.e. unique stationary distribution exists (Jackman 2009:183)
	
- Invariant measure (Jackman 2009:184)

- Reversibility (Jackman 2009:185)

************************************



\subsection{MH Algorithm}

% Who & when?
The \gls{mh} algorithm was first introduced by \textcite{metroplis1953} and generalized by \textcite{hastings1970}, henceforth being called the \textit{Metropolis Hastings} algorithm. 

************************************

On Metropolis 1953:
- Metropolis algorithm: sample from distribution with a Markov process (Liu 2001:105)
- algorithm used in varying scientific fields: biology, chemistry, computer science, economics, enginieering, material scienece, physics, statistics, etc. (Liu 2001:106)
- can sample from all distributions regardless of complexity and dimensionality (Liu 2001:106)
- problem: 
-- high correlation of samples (Liu 2001:106)
-- leads to higher variances of samples compared to variances of independent samples (Liu 2001:106)

basic idea:
-- simulate Markov chain in state space to sample from (Liu 2001:106), such 
	that the stationary distribution of this chain is the target distribution
-- note:
--- in Markov chain analysis we usually have a transition rule (Liu 2001:106) and searches
	for the stationary distribution (Liu 2001:106)
--- in MCMC simulations we know the stationary distribution, and we want to now an efficient
	transition rule (Liu 2001:106)
- i.e. "The Metropolis algorithm prescribes a transition rule for a Markov chain" (Liu 2001:111)


important: in Metropolis algorithm proposal function is necessarily symmetric

Hastings 1970 generalizes Metropolis' approach to non-symmetric cases, only restriction
on proposal function is $J(x,y) > 0 iff J(y,x) > 0 $(Liu 2001:111)


- retrieve information on random variable $\theta$ by sampling from its density $P(\theta)$ (Jackman 2009:201)
- generate Markov Chains with given target density, i.e. posterior density $P(\theta|D)$, as Markov chain's invariant density (Jackman 2009:201)
- i.e. correctly construction transition kernel of the chain (Jackman 2009:201)
- sampling from posterior density combining Monte Carlo principle with Markov chain theory is called MCMC (Jackman 2009:201)
- MH a core algorithm of MCMC (Jackman 2009:201)
- MH defines transition rules to generate Markov chain based on posterior density (Jackman 2009:201)


- originally proposed by Metropolis et al 1953 with acceptance ratio as:
$r_M = \frac{p(\theta^*|y)}{p(\theta^{t-1}|y)}$ (Jackman 2009:202)
- modified by Hastings 1970 to r from below (Jackman 2009:202)
- more acceptance ratios in Liu 2001:111f


% algorithm
Jackman 2009:201f
- sample $\theta$ values (states in Markov chain) from posterior distribution $P(\theta|D)$
- at iteration $t$, we are at state $\theta^{t-1}$
- make transition to state $\theta^t$ based on following algorithm:
- acceptance ratio \textit{r}

1. sample $\theta^*$ from \textit{proposal distribution}: $J_t(\theta^*, \theta^{t-1})$
2. $r \leftarrow \frac{P(\theta^*|y) J_t(\theta^*, \theta^{t-1})}{P(\theta^{t-1}|y) J_t(\theta^{t-1}, \theta^*)}$
3. $\alpha \leftarrow min(r, 1)$
4. sample $U ~ Ubif(0,1)$
5. \textbf{if} $U \leq \alpha$ \textbf{then}
6. $\theta^{(t)} \leftarrow \theta^*$
7. \textbf{else}
8. $\theta^{(t)} \leftarrow \theta^(t-1)$
9. \textbf{end if}


************************************

Assuming a target distribution $P(\theta)$, \gls{mh} samples values for $\theta$ at step $t$ in two stages (see e.g. \cite[][106f]{liu2001}): %first, propose a new value for $x$, second, decide whether to accept proposed change.
\begin{description}
\item[Proposal] Propose $\theta^'$, which is a random perturbation of the current value for $\theta^t$.  The proposed new state $\theta^'$ is generated from the \textit{proposal function} $J(x^{t}, x^{'})$. Calculate the difference between current and proposed state as $ r = x^{(t)} - x^{'} $.
\item[Decision] Draw a random number from a uniform distribution $U \sim Unif(0,1)$. TO COMPLETE
\end{description}


************************************


2. generate a random number; let $x^{(t+1)} = x^', if U \leq \frac{p(x^')}{p(x^{(t)})} \equiv exp()$
see  (Liu 2001:107) for second step, just type formula in paper
accept-reject rule in 2

************************************

Since the Markov chain is irreducible, the algorithm can start at any point and proceed by iteration proposal and decision until enough samples have been drawn.

************************************

- main features of Metropolis' sampling method (assume sample from distribution with density p(x)): (Hastings 1970:98)
	a. computation based on p(x) only through ratios if form p(x')/p(x) with sample points x', x
		- i.e. no normalizing constant
		- no factorization of p(x) needed
		- easy implementation in computer
	b. acquire sample sequcency by simulating Markov chain
		- i.e. resulting samples correlated
		- i.e. estimation of SD and error of an estimate might be more problematic than with independant samples
		
		


Metropolis-Hastings Algorithm (Liu 2001:111f):
- given current state$ x^(t)$
- draw y from proposal distribution$ J(x^(t), y)$
- draw$U ~ uniform(0,1)$ and update:
-- $x^{(t+1)} = y (if U \leq r(x^(t),y)) or x^t otherwise$

Metropolis and Hastings suggested using acceptance rate: $r(x,y)$ as
	$min(1, \frac{pr(y)*J(y,x)}{pr(x)}*J(x,y))$
(identical to original, if J is symmetric (Liu 2001:112))

Also other definitions for acceptance rate by Barker 1965 and charles stein (liu 112)

NOTE Liu 2001:118:
small step-size in proposal transition leads to slow movement in Markov chain;
large step-size in proposal transition leads to low acceptance rate
in both cases mixing rate of algorithm low

************************************



%\subsubsection{Excursion: Acceptance Ratios}
% maybe delte, see Jackman 2009: 203 and Liu 2001:111f

\subsection{Proposal Densities}\label{sec:densities}
% ca. 2,5 pages

% Setting:
% single parameter
% Kruschke example


\begin{enumerate}
\item Implement simple random walk presented by Kruschke
\item Use multivariate random walk from MCMCpack
\item Find Independence metropolis implementation: https://www.lancaster.ac.uk/pg/jamest/Group/stats4.html
\item use really bad one from HW 3?
\end{enumerate}





% relevance of proposal density
% discussion in other literature
% comparison random-walk and independence

- if J generates small values, acceptance rate is low, i.e. inefficient exploration of parameter space (Jackman 2009:202)
- if J generates high acceptance values, but only in small neighbourhood around $\theta^{t-1}$, again inefficient exploration of posterior

\paragraph{Random-Walk Metropolis}\label{sec:random}

Kruschke2014:149

% nature of random walk
% example implementations
% what implemented in app

\paragraph{Independence Metropolis}\label{sec:independence}

% nature of independence
% example implementations
% what implemented in app













\section{Shiny Hastings App}\label{sec:shiny}


\subsection{Interface}

\subsection{Settings}

\subsection{Posterior Densities}

% MH implementation using JAGS
% default parameters / settings
% proposal density packages
% evaluation
% shiny app
% github "publication"

% maybe implement settings such that specific characteristics of proposal densities can be shown!

% base: 
% single parameter
% base case of island hopping from Kruschke 2014! (own implementation)


\paragraph{Random-walk Metropolis}
Liu 2001:114f and Jackman 2009:?

\paragraph{Metropolized independent sampler}
hastings 1970,
liu2001:115f











\section{Conclusion}\label{sec:conclusion}
% ca. 1 page

\printbibliography[heading=bibintoc]\nocite{*}

\end{document}
